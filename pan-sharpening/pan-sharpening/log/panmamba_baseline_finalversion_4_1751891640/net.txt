Net(
  (pan_encoder): Sequential(
    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): HinResBlock(
      (identity): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))
      (conv_1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (relu_1): LeakyReLU(negative_slope=0.2)
      (conv_2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (relu_2): LeakyReLU(negative_slope=0.2)
      (norm): InstanceNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
    )
    (2): HinResBlock(
      (identity): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))
      (conv_1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (relu_1): LeakyReLU(negative_slope=0.2)
      (conv_2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (relu_2): LeakyReLU(negative_slope=0.2)
      (norm): InstanceNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
    )
    (3): HinResBlock(
      (identity): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))
      (conv_1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (relu_1): LeakyReLU(negative_slope=0.2)
      (conv_2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (relu_2): LeakyReLU(negative_slope=0.2)
      (norm): InstanceNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
    )
  )
  (ms_encoder): Sequential(
    (0): Conv2d(4, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): HinResBlock(
      (identity): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))
      (conv_1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (relu_1): LeakyReLU(negative_slope=0.2)
      (conv_2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (relu_2): LeakyReLU(negative_slope=0.2)
      (norm): InstanceNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
    )
    (2): HinResBlock(
      (identity): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))
      (conv_1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (relu_1): LeakyReLU(negative_slope=0.2)
      (conv_2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (relu_2): LeakyReLU(negative_slope=0.2)
      (norm): InstanceNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
    )
    (3): HinResBlock(
      (identity): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))
      (conv_1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (relu_1): LeakyReLU(negative_slope=0.2)
      (conv_2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (relu_2): LeakyReLU(negative_slope=0.2)
      (norm): InstanceNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
    )
  )
  (shallow_fusion1): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (shallow_fusion2): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (ms_to_token): PatchEmbed(
    (proj): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))
    (norm): LayerNorm(
      (body): BiasFree_LayerNorm()
    )
  )
  (pan_to_token): PatchEmbed(
    (proj): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))
    (norm): LayerNorm(
      (body): BiasFree_LayerNorm()
    )
  )
  (deep_fusion1): CrossMamba(
    (cross_mamba): Mamba(
      (in_proj): Linear(in_features=32, out_features=128, bias=False)
      (conv1d): Conv1d(64, 64, kernel_size=(4,), stride=(1,), padding=(3,), groups=64)
      (act): SiLU()
      (x_proj): Linear(in_features=64, out_features=34, bias=False)
      (dt_proj): Linear(in_features=2, out_features=64, bias=True)
      (conv1d_b): Conv1d(64, 64, kernel_size=(4,), stride=(1,), padding=(3,), groups=64)
      (x_proj_b): Linear(in_features=64, out_features=34, bias=False)
      (dt_proj_b): Linear(in_features=2, out_features=64, bias=True)
      (in_proj_extra): Linear(in_features=32, out_features=128, bias=False)
      (out_proj): Linear(in_features=64, out_features=32, bias=False)
    )
    (norm1): LayerNorm(
      (body): WithBias_LayerNorm()
    )
    (norm2): LayerNorm(
      (body): WithBias_LayerNorm()
    )
    (dwconv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32)
  )
  (deep_fusion2): CrossMamba(
    (cross_mamba): Mamba(
      (in_proj): Linear(in_features=32, out_features=128, bias=False)
      (conv1d): Conv1d(64, 64, kernel_size=(4,), stride=(1,), padding=(3,), groups=64)
      (act): SiLU()
      (x_proj): Linear(in_features=64, out_features=34, bias=False)
      (dt_proj): Linear(in_features=2, out_features=64, bias=True)
      (conv1d_b): Conv1d(64, 64, kernel_size=(4,), stride=(1,), padding=(3,), groups=64)
      (x_proj_b): Linear(in_features=64, out_features=34, bias=False)
      (dt_proj_b): Linear(in_features=2, out_features=64, bias=True)
      (in_proj_extra): Linear(in_features=32, out_features=128, bias=False)
      (out_proj): Linear(in_features=64, out_features=32, bias=False)
    )
    (norm1): LayerNorm(
      (body): WithBias_LayerNorm()
    )
    (norm2): LayerNorm(
      (body): WithBias_LayerNorm()
    )
    (dwconv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32)
  )
  (deep_fusion3): CrossMamba(
    (cross_mamba): Mamba(
      (in_proj): Linear(in_features=32, out_features=128, bias=False)
      (conv1d): Conv1d(64, 64, kernel_size=(4,), stride=(1,), padding=(3,), groups=64)
      (act): SiLU()
      (x_proj): Linear(in_features=64, out_features=34, bias=False)
      (dt_proj): Linear(in_features=2, out_features=64, bias=True)
      (conv1d_b): Conv1d(64, 64, kernel_size=(4,), stride=(1,), padding=(3,), groups=64)
      (x_proj_b): Linear(in_features=64, out_features=34, bias=False)
      (dt_proj_b): Linear(in_features=2, out_features=64, bias=True)
      (in_proj_extra): Linear(in_features=32, out_features=128, bias=False)
      (out_proj): Linear(in_features=64, out_features=32, bias=False)
    )
    (norm1): LayerNorm(
      (body): WithBias_LayerNorm()
    )
    (norm2): LayerNorm(
      (body): WithBias_LayerNorm()
    )
    (dwconv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32)
  )
  (deep_fusion4): CrossMamba(
    (cross_mamba): Mamba(
      (in_proj): Linear(in_features=32, out_features=128, bias=False)
      (conv1d): Conv1d(64, 64, kernel_size=(4,), stride=(1,), padding=(3,), groups=64)
      (act): SiLU()
      (x_proj): Linear(in_features=64, out_features=34, bias=False)
      (dt_proj): Linear(in_features=2, out_features=64, bias=True)
      (conv1d_b): Conv1d(64, 64, kernel_size=(4,), stride=(1,), padding=(3,), groups=64)
      (x_proj_b): Linear(in_features=64, out_features=34, bias=False)
      (dt_proj_b): Linear(in_features=2, out_features=64, bias=True)
      (in_proj_extra): Linear(in_features=32, out_features=128, bias=False)
      (out_proj): Linear(in_features=64, out_features=32, bias=False)
    )
    (norm1): LayerNorm(
      (body): WithBias_LayerNorm()
    )
    (norm2): LayerNorm(
      (body): WithBias_LayerNorm()
    )
    (dwconv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32)
  )
  (deep_fusion5): CrossMamba(
    (cross_mamba): Mamba(
      (in_proj): Linear(in_features=32, out_features=128, bias=False)
      (conv1d): Conv1d(64, 64, kernel_size=(4,), stride=(1,), padding=(3,), groups=64)
      (act): SiLU()
      (x_proj): Linear(in_features=64, out_features=34, bias=False)
      (dt_proj): Linear(in_features=2, out_features=64, bias=True)
      (conv1d_b): Conv1d(64, 64, kernel_size=(4,), stride=(1,), padding=(3,), groups=64)
      (x_proj_b): Linear(in_features=64, out_features=34, bias=False)
      (dt_proj_b): Linear(in_features=2, out_features=64, bias=True)
      (in_proj_extra): Linear(in_features=32, out_features=128, bias=False)
      (out_proj): Linear(in_features=64, out_features=32, bias=False)
    )
    (norm1): LayerNorm(
      (body): WithBias_LayerNorm()
    )
    (norm2): LayerNorm(
      (body): WithBias_LayerNorm()
    )
    (dwconv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32)
  )
  (pan_feature_extraction): Sequential(
    (0): SingleMambaBlock(
      (encoder): Mamba(
        (in_proj): Linear(in_features=32, out_features=128, bias=False)
        (conv1d): Conv1d(64, 64, kernel_size=(4,), stride=(1,), padding=(3,), groups=64)
        (act): SiLU()
        (x_proj): Linear(in_features=64, out_features=34, bias=False)
        (dt_proj): Linear(in_features=2, out_features=64, bias=True)
        (out_proj): Linear(in_features=64, out_features=32, bias=False)
      )
      (norm): LayerNorm(
        (body): WithBias_LayerNorm()
      )
    )
    (1): SingleMambaBlock(
      (encoder): Mamba(
        (in_proj): Linear(in_features=32, out_features=128, bias=False)
        (conv1d): Conv1d(64, 64, kernel_size=(4,), stride=(1,), padding=(3,), groups=64)
        (act): SiLU()
        (x_proj): Linear(in_features=64, out_features=34, bias=False)
        (dt_proj): Linear(in_features=2, out_features=64, bias=True)
        (out_proj): Linear(in_features=64, out_features=32, bias=False)
      )
      (norm): LayerNorm(
        (body): WithBias_LayerNorm()
      )
    )
    (2): SingleMambaBlock(
      (encoder): Mamba(
        (in_proj): Linear(in_features=32, out_features=128, bias=False)
        (conv1d): Conv1d(64, 64, kernel_size=(4,), stride=(1,), padding=(3,), groups=64)
        (act): SiLU()
        (x_proj): Linear(in_features=64, out_features=34, bias=False)
        (dt_proj): Linear(in_features=2, out_features=64, bias=True)
        (out_proj): Linear(in_features=64, out_features=32, bias=False)
      )
      (norm): LayerNorm(
        (body): WithBias_LayerNorm()
      )
    )
    (3): SingleMambaBlock(
      (encoder): Mamba(
        (in_proj): Linear(in_features=32, out_features=128, bias=False)
        (conv1d): Conv1d(64, 64, kernel_size=(4,), stride=(1,), padding=(3,), groups=64)
        (act): SiLU()
        (x_proj): Linear(in_features=64, out_features=34, bias=False)
        (dt_proj): Linear(in_features=2, out_features=64, bias=True)
        (out_proj): Linear(in_features=64, out_features=32, bias=False)
      )
      (norm): LayerNorm(
        (body): WithBias_LayerNorm()
      )
    )
    (4): SingleMambaBlock(
      (encoder): Mamba(
        (in_proj): Linear(in_features=32, out_features=128, bias=False)
        (conv1d): Conv1d(64, 64, kernel_size=(4,), stride=(1,), padding=(3,), groups=64)
        (act): SiLU()
        (x_proj): Linear(in_features=64, out_features=34, bias=False)
        (dt_proj): Linear(in_features=2, out_features=64, bias=True)
        (out_proj): Linear(in_features=64, out_features=32, bias=False)
      )
      (norm): LayerNorm(
        (body): WithBias_LayerNorm()
      )
    )
    (5): SingleMambaBlock(
      (encoder): Mamba(
        (in_proj): Linear(in_features=32, out_features=128, bias=False)
        (conv1d): Conv1d(64, 64, kernel_size=(4,), stride=(1,), padding=(3,), groups=64)
        (act): SiLU()
        (x_proj): Linear(in_features=64, out_features=34, bias=False)
        (dt_proj): Linear(in_features=2, out_features=64, bias=True)
        (out_proj): Linear(in_features=64, out_features=32, bias=False)
      )
      (norm): LayerNorm(
        (body): WithBias_LayerNorm()
      )
    )
    (6): SingleMambaBlock(
      (encoder): Mamba(
        (in_proj): Linear(in_features=32, out_features=128, bias=False)
        (conv1d): Conv1d(64, 64, kernel_size=(4,), stride=(1,), padding=(3,), groups=64)
        (act): SiLU()
        (x_proj): Linear(in_features=64, out_features=34, bias=False)
        (dt_proj): Linear(in_features=2, out_features=64, bias=True)
        (out_proj): Linear(in_features=64, out_features=32, bias=False)
      )
      (norm): LayerNorm(
        (body): WithBias_LayerNorm()
      )
    )
    (7): SingleMambaBlock(
      (encoder): Mamba(
        (in_proj): Linear(in_features=32, out_features=128, bias=False)
        (conv1d): Conv1d(64, 64, kernel_size=(4,), stride=(1,), padding=(3,), groups=64)
        (act): SiLU()
        (x_proj): Linear(in_features=64, out_features=34, bias=False)
        (dt_proj): Linear(in_features=2, out_features=64, bias=True)
        (out_proj): Linear(in_features=64, out_features=32, bias=False)
      )
      (norm): LayerNorm(
        (body): WithBias_LayerNorm()
      )
    )
  )
  (ms_feature_extraction): Sequential(
    (0): SingleMambaBlock(
      (encoder): Mamba(
        (in_proj): Linear(in_features=32, out_features=128, bias=False)
        (conv1d): Conv1d(64, 64, kernel_size=(4,), stride=(1,), padding=(3,), groups=64)
        (act): SiLU()
        (x_proj): Linear(in_features=64, out_features=34, bias=False)
        (dt_proj): Linear(in_features=2, out_features=64, bias=True)
        (out_proj): Linear(in_features=64, out_features=32, bias=False)
      )
      (norm): LayerNorm(
        (body): WithBias_LayerNorm()
      )
    )
    (1): SingleMambaBlock(
      (encoder): Mamba(
        (in_proj): Linear(in_features=32, out_features=128, bias=False)
        (conv1d): Conv1d(64, 64, kernel_size=(4,), stride=(1,), padding=(3,), groups=64)
        (act): SiLU()
        (x_proj): Linear(in_features=64, out_features=34, bias=False)
        (dt_proj): Linear(in_features=2, out_features=64, bias=True)
        (out_proj): Linear(in_features=64, out_features=32, bias=False)
      )
      (norm): LayerNorm(
        (body): WithBias_LayerNorm()
      )
    )
    (2): SingleMambaBlock(
      (encoder): Mamba(
        (in_proj): Linear(in_features=32, out_features=128, bias=False)
        (conv1d): Conv1d(64, 64, kernel_size=(4,), stride=(1,), padding=(3,), groups=64)
        (act): SiLU()
        (x_proj): Linear(in_features=64, out_features=34, bias=False)
        (dt_proj): Linear(in_features=2, out_features=64, bias=True)
        (out_proj): Linear(in_features=64, out_features=32, bias=False)
      )
      (norm): LayerNorm(
        (body): WithBias_LayerNorm()
      )
    )
    (3): SingleMambaBlock(
      (encoder): Mamba(
        (in_proj): Linear(in_features=32, out_features=128, bias=False)
        (conv1d): Conv1d(64, 64, kernel_size=(4,), stride=(1,), padding=(3,), groups=64)
        (act): SiLU()
        (x_proj): Linear(in_features=64, out_features=34, bias=False)
        (dt_proj): Linear(in_features=2, out_features=64, bias=True)
        (out_proj): Linear(in_features=64, out_features=32, bias=False)
      )
      (norm): LayerNorm(
        (body): WithBias_LayerNorm()
      )
    )
    (4): SingleMambaBlock(
      (encoder): Mamba(
        (in_proj): Linear(in_features=32, out_features=128, bias=False)
        (conv1d): Conv1d(64, 64, kernel_size=(4,), stride=(1,), padding=(3,), groups=64)
        (act): SiLU()
        (x_proj): Linear(in_features=64, out_features=34, bias=False)
        (dt_proj): Linear(in_features=2, out_features=64, bias=True)
        (out_proj): Linear(in_features=64, out_features=32, bias=False)
      )
      (norm): LayerNorm(
        (body): WithBias_LayerNorm()
      )
    )
    (5): SingleMambaBlock(
      (encoder): Mamba(
        (in_proj): Linear(in_features=32, out_features=128, bias=False)
        (conv1d): Conv1d(64, 64, kernel_size=(4,), stride=(1,), padding=(3,), groups=64)
        (act): SiLU()
        (x_proj): Linear(in_features=64, out_features=34, bias=False)
        (dt_proj): Linear(in_features=2, out_features=64, bias=True)
        (out_proj): Linear(in_features=64, out_features=32, bias=False)
      )
      (norm): LayerNorm(
        (body): WithBias_LayerNorm()
      )
    )
    (6): SingleMambaBlock(
      (encoder): Mamba(
        (in_proj): Linear(in_features=32, out_features=128, bias=False)
        (conv1d): Conv1d(64, 64, kernel_size=(4,), stride=(1,), padding=(3,), groups=64)
        (act): SiLU()
        (x_proj): Linear(in_features=64, out_features=34, bias=False)
        (dt_proj): Linear(in_features=2, out_features=64, bias=True)
        (out_proj): Linear(in_features=64, out_features=32, bias=False)
      )
      (norm): LayerNorm(
        (body): WithBias_LayerNorm()
      )
    )
    (7): SingleMambaBlock(
      (encoder): Mamba(
        (in_proj): Linear(in_features=32, out_features=128, bias=False)
        (conv1d): Conv1d(64, 64, kernel_size=(4,), stride=(1,), padding=(3,), groups=64)
        (act): SiLU()
        (x_proj): Linear(in_features=64, out_features=34, bias=False)
        (dt_proj): Linear(in_features=2, out_features=64, bias=True)
        (out_proj): Linear(in_features=64, out_features=32, bias=False)
      )
      (norm): LayerNorm(
        (body): WithBias_LayerNorm()
      )
    )
  )
  (swap_mamba1): TokenSwapMamba(
    (msencoder): Mamba(
      (in_proj): Linear(in_features=32, out_features=128, bias=False)
      (conv1d): Conv1d(64, 64, kernel_size=(4,), stride=(1,), padding=(3,), groups=64)
      (act): SiLU()
      (x_proj): Linear(in_features=64, out_features=34, bias=False)
      (dt_proj): Linear(in_features=2, out_features=64, bias=True)
      (out_proj): Linear(in_features=64, out_features=32, bias=False)
    )
    (panencoder): Mamba(
      (in_proj): Linear(in_features=32, out_features=128, bias=False)
      (conv1d): Conv1d(64, 64, kernel_size=(4,), stride=(1,), padding=(3,), groups=64)
      (act): SiLU()
      (x_proj): Linear(in_features=64, out_features=34, bias=False)
      (dt_proj): Linear(in_features=2, out_features=64, bias=True)
      (out_proj): Linear(in_features=64, out_features=32, bias=False)
    )
    (norm1): LayerNorm(
      (body): WithBias_LayerNorm()
    )
    (norm2): LayerNorm(
      (body): WithBias_LayerNorm()
    )
  )
  (swap_mamba2): TokenSwapMamba(
    (msencoder): Mamba(
      (in_proj): Linear(in_features=32, out_features=128, bias=False)
      (conv1d): Conv1d(64, 64, kernel_size=(4,), stride=(1,), padding=(3,), groups=64)
      (act): SiLU()
      (x_proj): Linear(in_features=64, out_features=34, bias=False)
      (dt_proj): Linear(in_features=2, out_features=64, bias=True)
      (out_proj): Linear(in_features=64, out_features=32, bias=False)
    )
    (panencoder): Mamba(
      (in_proj): Linear(in_features=32, out_features=128, bias=False)
      (conv1d): Conv1d(64, 64, kernel_size=(4,), stride=(1,), padding=(3,), groups=64)
      (act): SiLU()
      (x_proj): Linear(in_features=64, out_features=34, bias=False)
      (dt_proj): Linear(in_features=2, out_features=64, bias=True)
      (out_proj): Linear(in_features=64, out_features=32, bias=False)
    )
    (norm1): LayerNorm(
      (body): WithBias_LayerNorm()
    )
    (norm2): LayerNorm(
      (body): WithBias_LayerNorm()
    )
  )
  (patchunembe): PatchUnEmbed()
  (output): Refine(
    (conv_in): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (process): Sequential(
      (0): ChannelAttention(
        (avg_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_du): Sequential(
          (0): Conv2d(32, 8, kernel_size=(1, 1), stride=(1, 1))
          (1): ReLU(inplace=True)
          (2): Conv2d(8, 32, kernel_size=(1, 1), stride=(1, 1))
          (3): Sigmoid()
        )
        (process): Sequential(
          (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (1): ReLU()
          (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
      )
    )
    (conv_last): Conv2d(32, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  )
)
